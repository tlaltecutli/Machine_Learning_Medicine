{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1. Cluster and Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;[1.1. Clustering algorithms](#1.1.-Clustering-algorithms)\\\n",
    "&emsp;[1.2. K-Means Clustering](#1.2.-K-Means-Clustering)\\\n",
    "&emsp;[1.3. K-Median Clustering](#1.3.-K-Median-Clustering)\\\n",
    "&emsp;[1.4. Hierarchical Clustering](#1.4.-Hierarchical-Clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information taken from [Machine Learning Google](https://refer.is/ahoje1bi)\n",
    "\n",
    "**What is clustering?** \n",
    "\n",
    "Suppose you are working with a dataset that includes patient information from a healthcare system. The dataset is complex and includes both categorical and numeric features. You want to find patterns and similarities in the dataset. How might you approach this task?\n",
    "\n",
    "Clustering is an unsupervised machine learning technique designed to group unlabeled examples based on their similarity to each other. (If the examples are labeled, this kind of grouping is called classification.) Consider a hypothetical patient study designed to evaluate a new treatment protocol. During the study, patients report how many times per week they experience symptoms and the severity of the symptoms. Researchers can use clustering analysis to group patients with similar treatment responses into clusters. Figure 1 demonstrates one possible grouping of simulated data into three clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "  <img src = \"Images/clustering_example.png\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the unlabeled data on the left of Figure 1, you could guess that the data forms three clusters, even without a formal definition of similarity between data points. In real-world applications, however, you need to explicitly define a similarity measure, or the metric used to compare samples, in terms of the dataset's features. When examples have only a couple of features, visualizing and measuring similarity is straightforward. But as the number of features increases, combining and comparing features becomes less intuitive and more complex. Different similarity measures may be more or less appropriate for different clustering scenarios, and this course will address choosing an appropriate similarity measure in later sections: Manual similarity measures and Similarity measure from embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information taken from: [Clustering Scikit Learn](https://refer.is/1o5q7933)\n",
    "\n",
    "Clustering of unlabeled data can be performed with the module <ins>_sklearn.cluster_</ins>.\n",
    "\n",
    "Each clustering algorithm comes in two variants: a class, that implements the fit method to learn the clusters on train data, and a function, that, given train data, returns an array of integer labels corresponding to the different clusters. For the class, the labels over the training data can be found in the labels_ attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information taken from: [deepseek](https://chat.deepseek.com)\n",
    "\n",
    "Clustering in machine learning is an **unsupervised learning** technique used to group similar data points together based on their features or patterns. The goal is to partition a dataset into subsets (called clusters) such that:\n",
    "\n",
    "1. **Intra-cluster similarity**: Data points within the same cluster are as similar as possible.\n",
    "\n",
    "2. **Inter-cluster dissimilarity**: Data points from different clusters are as different as possible.\n",
    "\n",
    "Clustering is widely used in various applications, such as customer segmentation, image segmentation, anomaly detection, and document classification.\n",
    "\n",
    "Key Concepts in Clustering:\n",
    "\n",
    "1. **Cluster**: A group of data points that are similar to each other.\n",
    "\n",
    "2. **Centroid**: The center of a cluster (used in algorithms like K-Means).\n",
    "\n",
    "3. **Distance Metric**: A measure of similarity or dissimilarity between data points (e.g., Euclidean distance, cosine similarity).\n",
    "\n",
    "4. **Number of Clusters (k)**: The number of groups to form (often predefined or determined algorithmically).\n",
    "\n",
    "Common Clustering Algorithms:\n",
    "\n",
    "1. **K-Means Clustering**:\n",
    "\n",
    "* Divides data into k clusters by minimizing the variance within each cluster.\n",
    "\n",
    "* Requires the number of clusters (k) to be specified in advance.\n",
    "\n",
    "* Works well with spherical clusters of similar size.\n",
    "\n",
    "2. **Hierarchical Clustering**:\n",
    "\n",
    "* Builds a tree-like structure (dendrogram) to represent data points and their relationships.\n",
    "\n",
    "* Can be agglomerative (bottom-up) or divisive (top-down).\n",
    "\n",
    "* Does not require the number of clusters to be predefined.\n",
    "\n",
    "3. DBSCAN (Density-Based Spatial Clustering of Applications with Noise):\n",
    "\n",
    "* Groups data points based on density and identifies outliers as noise.\n",
    "\n",
    "* Does not require the number of clusters to be specified.\n",
    "\n",
    "* Can find clusters of arbitrary shapes.\n",
    "\n",
    "4. Gaussian Mixture Models (GMM):\n",
    "\n",
    "* Assumes data points are generated from a mixture of Gaussian distributions.\n",
    "\n",
    "* Provides probabilistic cluster assignments.\n",
    "\n",
    "5. Mean Shift Clustering:\n",
    "\n",
    "* Identifies clusters by finding dense regions in the data.\n",
    "\n",
    "* Does not require the number of clusters to be predefined.\n",
    "\n",
    "Applications of Clustering:\n",
    "\n",
    "* Customer Segmentation: Grouping customers based on purchasing behavior.\n",
    "\n",
    "* Image Segmentation: Dividing an image into regions with similar pixel values.\n",
    "\n",
    "* Anomaly Detection: Identifying outliers or unusual patterns in data.\n",
    "\n",
    "* Document Clustering: Grouping similar documents or articles together.\n",
    "\n",
    "* Biology: Classifying genes or proteins with similar functions.\n",
    "\n",
    "Challenges in Clustering:\n",
    "\n",
    "* Choosing the Right Number of Clusters: Determining the optimal number of clusters can be difficult.\n",
    "\n",
    "* Scalability: Some algorithms struggle with large datasets.\n",
    "\n",
    "* Sensitivity to Initial Conditions: Algorithms like K-Means can produce different results based on initial centroids.\n",
    "\n",
    "* Handling Noise and Outliers: Some algorithms (e.g., K-Means) are sensitive to outliers.\n",
    "\n",
    "Clustering is a powerful tool for discovering hidden structures in data and is a fundamental technique in machine learning and data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Clustering algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning datasets can have millions of examples, but not all clustering algorithms scale efficiently. Many clustering algorithms compute the similarity between all pairs of examples, which means their runtime increases as the square of the number of examples, denoted as $O(n^2)$ in complexity notation. $O(n^2)$ algorithms are not practical for datasets with millions of examples.\n",
    "\n",
    "The k-means algorithm has a complexity of $O(n)$, meaning that the algorithm scales linearly with $n$. This algorithm will be the focus of this course.\n",
    "\n",
    "Types of clustering:\n",
    "\n",
    "1. Centroid-based clustering\n",
    "2. Density-based clustering\n",
    "3. Distribution-based clustering\n",
    "4. Hierarchical clustering\n",
    "\n",
    "For an exhaustive list of different approaches to clustering, see _A Comprehensive Survey of Clustering Algorithms Xu, D. & Tian, Y. Ann. Data. Sci. (2015) 2: 165_. Each approach is best suited to a particular data distribution. This course briefly discusses four common approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A popular **clustering** algorithm that groups examples in unsupervised learning. The k-means algorithm basically does the following:\n",
    "\n",
    "Iteratively determines the best k center points (known as **centroids**).\n",
    "Assigns each example to the closest centroid. Those examples nearest the same centroid belong to the same group.\n",
    "The k-means algorithm picks centroid locations to minimize the cumulative square of the distances from each example to its closest centroid.\n",
    "\n",
    "For example, consider the following plot of dog height to dog width:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "  <img src = \"Images/DogDimensions.svg\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **KMeans** algorithm clusters data by trying to separate samples in n groups of equal variance, minimizing a criterion known as the inertia or within-cluster sum-of-squares (see below). This algorithm requires the number of clusters to be specified. It scales well to large numbers of samples and has been used across a large range of application areas in many different fields.\n",
    "\n",
    "The k-means algorithm divides a set of **$N$** samples **$X$** into **$K$** disjoint clusters **$C$**, each described by the mean **$\\mu_j$** of the samples in the cluster. The means are commonly called the cluster “centroids”; note that they are not, in general, points from $**X**$, although they live in the same space.\n",
    "\n",
    "The K-means algorithm aims to choose centroids that minimise the **inertia**, or **within-cluster sum-of-squares criterion**:\n",
    "\n",
    "$$\n",
    "\\sum_{i=0}^{n}{\\min_{\\mu_j \\in C}(||x_i - \\mu_j||^2)}\n",
    "$$\n",
    "\n",
    "Inertia can be recognized as a measure of how internally coherent clusters are. It suffers from various drawbacks:\n",
    "\n",
    "- Inertia makes the assumption that clusters are convex and isotropic, which is not always the case. It responds poorly to elongated clusters, or manifolds with irregular shapes.\n",
    "\n",
    "- Inertia is not a normalized metric: we just know that lower values are better and zero is optimal. But in very high-dimensional spaces, Euclidean distances tend to become inflated (this is an instance of the so-called “curse of dimensionality”). Running a dimensionality reduction algorithm such as Principal component analysis (PCA) prior to k-means clustering can alleviate this problem and speed up the computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. K-Median Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A clustering algorithm closely related to **k-means**. The practical difference between the two is as follows:\n",
    "* In k-means, centroids are determined by minimizing the sum of the squares of the distance between a centroid candidate and each of its examples.\n",
    "* In k-median, centroids are determined by minimizing the sum of the distance between a centroid candidate and each of its examples.\n",
    "\n",
    "Note that the definitions of distance are also different:\n",
    "* k-means relies on the Euclidean distance from the centroid to an example. (In two dimensions, the Euclidean distance means using the Pythagorean theorem to calculate the hypotenuse.) For example, the k-means distance between (2,2) and (5,-2) would be:\n",
    "\n",
    "$$\n",
    "Euclidian \\text{ } distance = \\sqrt{(2 - 5)^2 + (2 - -2)^2} = 5\n",
    "$$ (euclidian_distance)\n",
    "\n",
    "* k-median relies on the Manhattan distance from the centroid to an example. This distance is the sum of the absolute deltas in each dimension. For example, the k-median distance between (2,2) and (5,-2) would be:\n",
    "\n",
    "$$\n",
    "Manhattan \\text{ } distance = |2 - 5| + |2 - -2| = 7\n",
    "$$ (euclidian_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical clustering is a general family of clustering algorithms that build nested clusters by merging or splitting them successively. This hierarchy of clusters is represented as a tree (or dendrogram). The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample. See the Wikipedia page for more details.\n",
    "\n",
    "The AgglomerativeClustering object performs a hierarchical clustering using a bottom up approach: each observation starts in its own cluster, and clusters are successively merged together. The linkage criteria determines the metric used for the merge strategy:\n",
    "\n",
    "Ward minimizes the sum of squared differences within all clusters. It is a variance-minimizing approach and in this sense is similar to the k-means objective function but tackled with an agglomerative hierarchical approach.\n",
    "\n",
    "Maximum or complete linkage minimizes the maximum distance between observations of pairs of clusters.\n",
    "\n",
    "Average linkage minimizes the average of the distances between all observations of pairs of clusters.\n",
    "\n",
    "Single linkage minimizes the distance between the closest observations of pairs of clusters.\n",
    "\n",
    "AgglomerativeClustering can also scale to large number of samples when it is used jointly with a connectivity matrix, but is computationally expensive when no connectivity constraints are added between samples: it considers at each step all the possible merges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
